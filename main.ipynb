{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommender ANNOY\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import random\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "eval_path = 'data/audioset_v1_embeddings/eval/'\n",
    "bal_train = 'data/audioset_v1_embeddings/bal_train/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Train + Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files with relative paths and proper extension\n",
    "audio_test_dataset = glob.glob(eval_path+\"*.tfrecord\")\n",
    "audio_train_dataset = glob.glob(bal_train+\"*.tfrecord\")\n",
    "\n",
    "# Convert audio_test_dataset to tf record dataset object\n",
    "tf_test_dataset = tf.data.TFRecordDataset(audio_test_dataset)\n",
    "del audio_test_dataset\n",
    "tf_train_dataset = tf.data.TFRecordDataset(audio_train_dataset)\n",
    "del audio_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Labels class indices\n",
    "class_labels_all = pd.read_csv(\"./data/class_labels_indices.csv\")\n",
    "class_labels_train = pd.read_csv(\"./data/balanced_train_segments.csv\",  sep=', ', engine='python')\n",
    "class_labels_test = pd.read_csv(\"./data/eval_segments.csv\", sep=', ', engine='python')\n",
    "\n",
    "# Format 'positive_labels\" on train and test df\n",
    "class_labels_train['positive_labels'] = class_labels_train['positive_labels'].str.replace('\"','').str.split(',')\n",
    "class_labels_test['positive_labels'] = class_labels_test['positive_labels'].str.replace('\"','').str.split(',')\n",
    "\n",
    "# Merge Display Names on Train and Test DF\n",
    "class_labels_train['display_name'] = class_labels_train.apply(\n",
    "    lambda row : ','.join([class_labels_all.loc[class_labels_all['mid'] == x, 'display_name'].values[0] for x in row['positive_labels']]), axis = 1)\n",
    "\n",
    "class_labels_test['display_name'] = class_labels_test.apply(\n",
    "    lambda row : ','.join([class_labels_all.loc[class_labels_all['mid'] == x, 'display_name'].values[0] for x in row['positive_labels']]), axis = 1)\n",
    "\n",
    "\n",
    "# raise SystemExit(\"Stop right there!\")\n",
    "# Select only Music from class_labels (there are different classes)\n",
    "music_labels_train = class_labels_train[class_labels_train['display_name'].str.contains('Music', case=False)]\n",
    "music_labels_test = class_labels_test[class_labels_test['display_name'].str.contains('Music', case=False)]\n",
    "del class_labels_test\n",
    "del class_labels_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert *.tfrecord to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract *.tfrecord data\n",
    "def extract_tfrecord_dictionary(tf_iterator,youtubeID_filter, NUM_SECONDS = 10):\n",
    "    dataset_audios = []\n",
    "    for raw_file in tf_iterator:\n",
    "        # Extract File\n",
    "        extracted_file = tf.train.SequenceExample()\n",
    "        extracted_file.ParseFromString(raw_file.numpy())\n",
    "\n",
    "        # Get Audio Metadata\n",
    "        single_audio = {}\n",
    "        single_audio['label'] = extracted_file.context.feature['labels'].int64_list.value\n",
    "        single_audio['video_id'] = str(extracted_file.context.feature['video_id'].bytes_list.value[0]).replace(\"'\",' ').split()[-1]\n",
    "        single_audio['start_time'] = extracted_file.context.feature['start_time_seconds'].float_list.value[0]\n",
    "        single_audio['end_time'] = extracted_file.context.feature['end_time_seconds'].float_list.value[0]\n",
    "\n",
    "        # Get audio_embedding\n",
    "        full_audio_embedding = extracted_file.feature_lists.feature_list['audio_embedding'].feature\n",
    "        feature_list = [list(feature.bytes_list.value[0]) for feature in full_audio_embedding]\n",
    "        single_audio['data'] = [byte for sublist in feature_list[:NUM_SECONDS] for byte in sublist]\n",
    "\n",
    "        # Making sure each data vector has the same 128 size\n",
    "        if((len(single_audio['data']) != (128 * NUM_SECONDS)) or (single_audio['video_id'] not in youtubeID_filter)):\n",
    "            continue\n",
    "        \n",
    "        # Append single_audio dict\n",
    "        dataset_audios.append(single_audio)\n",
    "\n",
    "    return dataset_audios\n",
    "\n",
    "NUM_SECONDS = 10\n",
    "dataset_final_test = extract_tfrecord_dictionary(tf_test_dataset,music_labels_test['YTID'].values, NUM_SECONDS)\n",
    "dataset_final_train = extract_tfrecord_dictionary(tf_train_dataset,music_labels_train['YTID'].values, NUM_SECONDS)\n",
    "\n",
    "# convert into json file\n",
    "with open(\"audio_preprocessed_test.json\", \"w\") as jsonFile:\n",
    "   json.dump(repr(dataset_final_test), jsonFile)\n",
    "with open(\"audio_preprocessed_train.json\", \"w\") as jsonFile:\n",
    "   json.dump(repr(dataset_final_train), jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json-file (To avoid re-process)\n",
    "dataset_final_train = None\n",
    "dataset_final_test = None\n",
    "with open(\"audio_preprocessed_train.json\", \"rb\") as jsonFile:\n",
    "   # Read Data\n",
    "   json_data = json.loads(jsonFile.read())\n",
    "   # Convert to list of dicts\n",
    "   dataset_final_train = literal_eval(json_data)\n",
    "with open(\"audio_preprocessed_test.json\", \"rb\") as jsonFile:\n",
    "   # Read Data\n",
    "   json_data = json.loads(jsonFile.read())\n",
    "   # Convert to list of dicts\n",
    "   dataset_final_test = literal_eval(json_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNOY Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUDIO_DIM = 128*NUM_SECONDS\n",
    "\n",
    "# Create annoy index\n",
    "annoyModel= AnnoyIndex(AUDIO_DIM, 'angular')\n",
    "\n",
    "# Run only on all parameters\n",
    "for annoy_idx, single_audio_train in enumerate(dataset_final_train):\n",
    "    # Get embedded audio data\n",
    "    audio_data_train = single_audio_train['data']\n",
    "    # Add to Annoy index\n",
    "    annoyModel.add_item(annoy_idx, audio_data_train)\n",
    "\n",
    "# Build Annoy\n",
    "annoyModel.build(n_trees=5000)\n",
    "# Save nearest neighbor model\n",
    "annoyModel.save('nearest_neighbor_graph.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Vector\n",
      "Labels: [152, 154, 160] ['Electric piano,Harpsichord,Keyboard (musical)'] - Video ID: 2laUv3y7OfA\n",
      "\n",
      "Youtube URL: http://www.youtube.com/watch?v=2laUv3y7OfA?&start=30&end=40\n",
      "\n",
      "....................................................................................\n",
      "SONG RECOMMENDATIONS\n",
      "\n",
      "Annoy Index: 998 - Labels: ['Harpsichord,Keyboard (musical),Piano'] - Video ID: CeW3NLlstf4 - Start: 170.0 - End: 180.0\n",
      "Youtube URL: http://www.youtube.com/watch?v=CeW3NLlstf4?&start=170&end=180\n",
      "\n",
      "Annoy Index: 2273 - Labels: ['Cello,Music,Musical instrument,Orchestra,Violin, fiddle,Classical music,Bowed string instrument'] - Video ID: yrme-KRBvzk - Start: 50.0 - End: 60.0\n",
      "Youtube URL: http://www.youtube.com/watch?v=yrme-KRBvzk?&start=50&end=60\n",
      "\n",
      "Annoy Index: 1500 - Labels: ['Harpsichord,Keyboard (musical)'] - Video ID: L1n4TWq2ZO8 - Start: 30.0 - End: 40.0\n",
      "Youtube URL: http://www.youtube.com/watch?v=L1n4TWq2ZO8?&start=30&end=40\n",
      "\n",
      "Annoy Index: 793 - Labels: ['Harpsichord,Keyboard (musical)'] - Video ID: 8yiJOImd6k0 - Start: 30.0 - End: 40.0\n",
      "Youtube URL: http://www.youtube.com/watch?v=8yiJOImd6k0?&start=30&end=40\n",
      "\n",
      "Annoy Index: 1231 - Labels: ['Music,Bluegrass'] - Video ID: g5FcJLulKAk - Start: 30.0 - End: 40.0\n",
      "Youtube URL: http://www.youtube.com/watch?v=g5FcJLulKAk?&start=30&end=40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Usage\n",
    "# Get 5 Nearest Neighbors of test data sample\n",
    "\n",
    "# Get Random Element\n",
    "sample_test = random.choice(dataset_final_test) \n",
    "# Get Element audio Data\n",
    "sample_vector_test = sample_test['data']\n",
    "\n",
    "# Test Data\n",
    "print(\"Test Vector\")\n",
    "labels_test = music_labels_test.loc[music_labels_test['YTID'] == sample_test['video_id'], 'display_name'].values\n",
    "print(f\"Labels: {sample_test['label']} {labels_test} - Video ID: {sample_test['video_id']}\\n\")\n",
    "print(f\"Youtube URL: http://www.youtube.com/watch?v={sample_test['video_id']}?&start={int(sample_test['start_time'])}&end={int(sample_test['end_time'])}\\n\")\n",
    "print(\"....................................................................................\")\n",
    "\n",
    "# Get Nearest Neighbors from Annoy Model\n",
    "nn_annoy = annoyModel.get_nns_by_vector(sample_vector_test, 5)\n",
    "\n",
    "# Get The Nearest Neighbors Information\n",
    "print('SONG RECOMMENDATIONS\\n')\n",
    "for i in nn_annoy:\n",
    "    # We get the Reference from the training set\n",
    "    sample = dataset_final_train[i]\n",
    "    # Extract Classes\n",
    "    music_labels_sample = music_labels_train.loc[music_labels_train['YTID'] == sample['video_id'], 'display_name'].values\n",
    "    print(f\"Annoy Index: {i} - Labels: {music_labels_sample} - Video ID: {sample['video_id']} - Start: {sample['start_time']} - End: {sample['end_time']}\")\n",
    "    # formated_start = \n",
    "    # formated_end = \n",
    "    print(f\"Youtube URL: http://www.youtube.com/watch?v={sample['video_id']}?&start={int(sample['start_time'])}&end={int(sample['end_time'])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RECO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4c24091ed06221423e8561eb1fe2163963ae1c4024395f0d18a39948ee283d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
